{
  "job":
  {
    "name": "ollama_inference_job",
    "infrastructure": {
      "partition": "cpu",
      "account": "p200981",
      "nodes": 1,
      "mem_gb": 64,
      "time": "01:00:00"
    },
    "service": {
      "type": "inference",
      "model": "llama2",
      "precision": "fp16",
      "n_clients": 100,
      "n_requests_per_client": 10
    }
  }
}