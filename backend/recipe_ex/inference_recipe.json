{
  "job":
  {
    "name": "ollama_inference_job",
    "infrastructure": {
      "partition": "gpu",
      "account": "p200981",
      "nodes": 1,
      "mem_gb": 64,
      "time": "00:15:00"
    },
    "service": {
      "type": "inference",
      "model": "llama2",
      "precision": "fp16",
      "n_clients": 2,
      "n_requests_per_client": 5
    }
  }
}